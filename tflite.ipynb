{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from glob import glob\n",
    "import os\n",
    "import argparse\n",
    "from tensorflow.keras.models import load_model\n",
    "from clean import downsample_mono, envelope\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import argparse\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input details:\n",
      "[{'name': 'stft_1_input', 'index': 0, 'shape': array([    1, 40000,     1], dtype=int32), 'shape_signature': array([   -1, 40000,     1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
      "\n",
      "Output details:\n",
      "[{'name': 'Identity', 'index': 138, 'shape': array([ 1, 44], dtype=int32), 'shape_signature': array([-1, 44], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Location of tflite model file (float32 or int8 quantized)\n",
    "model_path = \"./model.tflite\"\n",
    "\n",
    "# Processed features (copy from Edge Impulse project)\n",
    "features = [\n",
    "  # <COPY FEATURES HERE!>\n",
    "]\n",
    "  \n",
    "# Load TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Allocate tensors\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Print the input and output details of the model\n",
    "print()\n",
    "print(\"Input details:\")\n",
    "print(input_details)\n",
    "print()\n",
    "print(\"Output details:\")\n",
    "print(output_details)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(args):\n",
    "\n",
    "    # Load TFLite model and allocate tensors.\n",
    "    interpreter = tf.lite.Interpreter(model_path=args.model_fn)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    # Get input and output tensors information\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    wav_paths = glob('{}/**'.format(args.src_dir), recursive=True)\n",
    "    wav_paths = sorted([x.replace(os.sep, '/') for x in wav_paths if '.wav' in x])\n",
    "    classes = sorted(os.listdir(args.src_dir))\n",
    "    labels = [os.path.split(x)[0].split('/')[-1] for x in wav_paths]\n",
    "    le = LabelEncoder()\n",
    "    y_true = le.fit_transform(labels)\n",
    "    results = []\n",
    "\n",
    "    for z, wav_fn in tqdm(enumerate(wav_paths), total=len(wav_paths)):\n",
    "        rate, wav = downsample_mono(wav_fn, args.sr)\n",
    "        mask, env = envelope(wav, rate, threshold=args.threshold)\n",
    "        clean_wav = wav[mask]\n",
    "        step = int(args.sr*args.dt)\n",
    "        batch = []\n",
    "\n",
    "        for i in range(0, clean_wav.shape[0], step):\n",
    "            sample = clean_wav[i:i+step]\n",
    "            sample = sample.reshape(-1, 1)\n",
    "            if sample.shape[0] < step:\n",
    "                tmp = np.zeros(shape=(step, 1), dtype=np.float32)\n",
    "                tmp[:sample.shape[0],:] = sample.flatten().reshape(-1, 1)\n",
    "                sample = tmp\n",
    "            batch.append(sample)\n",
    "\n",
    "        X_batch = np.array(batch, dtype=np.float32)\n",
    "\n",
    "        # Use the interpreter to predict\n",
    "        y_pred = []\n",
    "        for x in X_batch:\n",
    "            interpreter.set_tensor(input_details[0]['index'], [x])\n",
    "            interpreter.invoke()\n",
    "            output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "            print(\"output_data\")\n",
    "            \n",
    "            print(output_data)\n",
    "            y_pred.append(output_data)\n",
    "\n",
    "        y_mean = np.mean(y_pred, axis=0)\n",
    "        print(\"y_mean\")\n",
    "        \n",
    "        print(y_mean)\n",
    "        y_pred = np.argmax(y_mean)\n",
    "        real_class = os.path.dirname(wav_fn).split('/')[-1]\n",
    "        print('Actual class: {}, Predicted class: {}'.format(real_class, classes[y_pred]))\n",
    "        results.append(y_mean)\n",
    "\n",
    "    np.save(os.path.join('logs', args.pred_fn), np.array(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2691 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Audio Classification Training')\n",
    "parser.add_argument('--model_fn', type=str, default='model.tflite',\n",
    "                    help='model file to make predictions')\n",
    "parser.add_argument('--pred_fn', type=str, default='tflite_pred',\n",
    "                    help='fn to write predictions in logs dir')\n",
    "parser.add_argument('--src_dir', type=str, default='wavfiles',\n",
    "                    help='directory containing wavfiles to predict')\n",
    "parser.add_argument('--dt', type=float, default=2.5,\n",
    "                    help='time in seconds to sample audio')\n",
    "parser.add_argument('--sr', type=int, default=16000,\n",
    "                    help='sample rate of clean audio')\n",
    "parser.add_argument('--threshold', type=str, default=20,\n",
    "                    help='threshold magnitude for np.int16 dtype')\n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "make_prediction(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
